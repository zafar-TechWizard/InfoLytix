[
    {
        "url": "https://adasci.org/product/unpacking-parallelism-practical-strategies-for-scaling-ai-workflows/",
        "title": "[Upcoming Webinar] Unpacking Parallelism: Practical Strategies for Scaling AI Workflows - Association of Data Scientists",
        "documents": [
            "[Upcoming Webinar] Unpacking Parallelism: Practical Strategies for Scaling AI Workflows - Association of Data Scientists Skip to content Memberships Close Memberships Open Memberships Individual Membership Join the world‚Äôs leading Data Science professional community. You can access both General & Premium Memberships. Learn More Corporate Membership Any corporate, organization or academic institution having common interests in the AI field can become a member of ADaSci. Learn More Accreditations Close Accreditations Open Accreditations Chartered Data Scientist‚Ñ¢ The Chartered Data Scientist (CDS) credential gives a strong understanding of advanced data science profession and in-depth, applied analytics skills. Learn More Certified Data Scientist - Associate Level Best suitable for the aspirants who want to start their career in the data science field, this certification. Learn More Certified Generative AI Engineer An upskilling-linked certification initiative designed to recognize talent",
            "skills. Learn More Certified Data Scientist - Associate Level Best suitable for the aspirants who want to start their career in the data science field, this certification. Learn More Certified Generative AI Engineer An upskilling-linked certification initiative designed to recognize talent in generative AI and large language models. Learn More Continuous Learning Close Continuous Learning Open Continuous Learning Our Latest Courses [Upcoming Workshop] Building AI Solutions with DeepSeek: A Hands-On Workshop ‚Çπ 1,743.00 Add to cart Generative AI-Powered Software Engineering with AI Coding Assistants ‚Çπ 5,226.00 Add to cart NVIDIA Inference Microservices (NIM) Mastery Course ‚Çπ 8,712.00 Add to cart Building Generative AI Applications with Amazon Bedrock ‚Çπ 5,226.00 Add to cart Hi, Welcome back! Keep me signed in Forgot Password? Sign In Don't have an account? Register Now Access all Courses Corporate Trainings Contact ‚Çπ 0.00 0 Cart Members Area Home / Continuous Learning /¬†[Upcoming",
            "to cart Building Generative AI Applications with Amazon Bedrock ‚Çπ 5,226.00 Add to cart Hi, Welcome back! Keep me signed in Forgot Password? Sign In Don't have an account? Register Now Access all Courses Corporate Trainings Contact ‚Çπ 0.00 0 Cart Members Area Home / Continuous Learning /¬†[Upcoming Webinar] Unpacking Parallelism: Practical Strategies for Scaling AI Workflows [Upcoming Webinar] Unpacking Parallelism: Practical Strategies for Scaling AI Workflows ‚Çπ 1,743.00 Original price was: ‚Çπ1,743.00. ‚Çπ 0.00 Current price is: ‚Çπ0.00. Shashank Kapadia is a seasoned ML Engineer specializing in scalable AI solutions, NLP, and ethical AI practices. [Upcoming Webinar] Unpacking Parallelism: Practical Strategies for Scaling AI Workflows quantity Add to cart Description Description Join Shashank Kapadia, Staff Machine Learning Engineer at Walmart Global Tech, for a 1.5-hour webinar on ‚ÄúUnpacking Parallelism: Practical Strategies for Scaling AI Workflows.‚Äù Learn how to optimize AI workflows,",
            "Strategies for Scaling AI Workflows quantity Add to cart Description Description Join Shashank Kapadia, Staff Machine Learning Engineer at Walmart Global Tech, for a 1.5-hour webinar on ‚ÄúUnpacking Parallelism: Practical Strategies for Scaling AI Workflows.‚Äù Learn how to optimize AI workflows, overcome scaling challenges, and implement parallelism techniques using distributed training, cloud infrastructure, and real-world case studies to enhance your AI systems‚Äô performance. Agenda: Introduction to Parallelism in AI Workflows Challenges in Scaling AI Workflows Key Strategies for Implementing Parallelism in AI Systems Distributed Training: Techniques and Tools Scaling AI Workflows with Cloud Computing and GPUs Real-World Case Studies and Applications About the speaker: Shashank Kapadia, Staff Machine Learning Engineer at Walmart Global Tech Shashank Kapadia is a seasoned Machine Learning Engineer with over a decade of experience in MLOps, NLP, Recommendation Systems, and LLMs. He",
            "Cloud Computing and GPUs Real-World Case Studies and Applications About the speaker: Shashank Kapadia, Staff Machine Learning Engineer at Walmart Global Tech Shashank Kapadia is a seasoned Machine Learning Engineer with over a decade of experience in MLOps, NLP, Recommendation Systems, and LLMs. He specializes in building scalable AI solutions, driving ML projects from ideation to deployment, and leading cross-functional teams. Passionate about ethical AI, he actively mentors and shares insights through writing. With expertise in TensorFlow, PyTorch, and cloud technologies like GCP and AWS, Shashank excels in transforming AI workflows for enhanced efficiency and user interaction. [Upcoming Webinar] Unpacking Parallelism: Practical Strategies for Scaling AI Workflows Category Continuous Learning Related products Generative AI Hands-On Course: Achieving Hyperpersonalization in BFSI ‚Çπ 2,613.00 Add to cart CDS Video Series | Section 04: Supervised and Unsupervised Learning ‚Çπ 3,920.00 Add",
            "Webinar] Unpacking Parallelism: Practical Strategies for Scaling AI Workflows Category Continuous Learning Related products Generative AI Hands-On Course: Achieving Hyperpersonalization in BFSI ‚Çπ 2,613.00 Add to cart CDS Video Series | Section 04: Supervised and Unsupervised Learning ‚Çπ 3,920.00 Add to cart Build your own Generative Adversarial Networks (GANs) from Scratch ‚Çπ 3,049.00 Add to cart CDS Video Series | Sec08. Deployment and Model Management ‚Çπ 4,791.00 Add to cart Not a member, but still want to know what we are upto? Subscribe to our Newsletter Email Start Free Trial The power of intelligence to propel humanity and make a difference Our Accrediations Chartered Data Scientist‚Ñ¢ (CDS) Certified Data Scientist - Associate Level Certified Generative AI Engineer CDS Program About CDS Exam Information Candidate Body of Knowledge (CBOK) Exam Structure Exam Cost and Registration Fees Ethical & Standards for Chartered Data Scientists (CDS) How to Earn the CDS Charter Terms &",
            "(CDS) Certified Data Scientist - Associate Level Certified Generative AI Engineer CDS Program About CDS Exam Information Candidate Body of Knowledge (CBOK) Exam Structure Exam Cost and Registration Fees Ethical & Standards for Chartered Data Scientists (CDS) How to Earn the CDS Charter Terms & Conditions For CDS‚Ñ¢ Membership Individual Membership Institutional Membership About About ADaSci Continuous Learning Team Privacy Policy Terms and Conditions Chapters Blogs Contact For Organizations Corporate Trainings CDS for Organizations Corporate Membership Journal Lattice About Review Committee Twitter Facebook-f Linkedin ¬© 2024 All rights reserved Association of Data Scientists We noticed you're visiting from India. We've updated our prices to Indian rupee for your shopping convenience. Use United States (US) dollar instead. Dismiss"
        ]
    },
    {
        "url": "https://python.langchain.com/v0.1/docs/use_cases/extraction/quickstart/",
        "title": "Quickstart | ü¶úÔ∏èüîó LangChain",
        "documents": [
            "Quickstart | ü¶úÔ∏èüîó LangChain Skip to main content This is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here . Components Integrations Guides API Reference More People Versioning Contributing Templates Cookbooks Tutorials YouTube v0.1 Latest v0.2 v0.1 ü¶úÔ∏èüîó LangSmith LangSmith Docs LangServe GitHub Templates GitHub Templates Hub LangChain Hub JS/TS Docs üí¨ Search Get started Introduction Quickstart Installation Use cases Q&A with RAG Extracting structured output Quickstart Guidelines Use Reference Examples More Chatbots Tool use and agents Query analysis Q&A over SQL + CSV More Expression Language Get started Runnable interface Primitives Advantages of LCEL Streaming Add message history (memory) More Ecosystem ü¶úüõ†Ô∏è LangSmith ü¶úüï∏Ô∏è LangGraph ü¶úÔ∏èüèì LangServe Security This is documentation for LangChain v0.1 , which is no longer actively maintained. For the current stable version, see this version ( Latest ). Use cases Extracting structured output Quickstart On this page Quickstart In this quick start, we will use chat models that are capable of function/tool calling to extract information from text. info Extraction using function/tool calling only works with models that support function/tool calling . Set up ‚Äã We will use the structured output method available on LLMs that are capable of function/tool calling . Select a model, install the dependencies for it and set up API keys! !pip install langchain # Install a model",
            "information from text. info Extraction using function/tool calling only works with models that support function/tool calling . Set up ‚Äã We will use the structured output method available on LLMs that are capable of function/tool calling . Select a model, install the dependencies for it and set up API keys! !pip install langchain # Install a model capable of tool calling # pip install langchain-openai # pip install langchain-mistralai # pip install langchain-fireworks # Set env vars for the relevant model or load from a .env file: # import dotenv # dotenv.load_dotenv() The Schema ‚Äã First, we need to describe what information we want to extract from the text. We'll use Pydantic to define an example schema  to extract personal information. from typing import Optional from langchain_core . pydantic_v1 import BaseModel , Field class Person ( BaseModel ) : \"\"\"Information about a person.\"\"\" # ^ Doc-string for the entity Person. # This doc-string is sent to the LLM as the description of the schema Person, # and it can help to improve extraction results. # Note that: # 1. Each field is an `optional` -- this allows the model to decline to extract it! # 2. Each field has a `description` -- this description is used by the LLM. # Having a good description can help improve extraction results. name : Optional [ str ] = Field ( default = None , description = \"The name of the person\" ) hair_color : Optional [ str ] = Field ( default = None , description = \"The color of the peron's hair if",
            "extract it! # 2. Each field has a `description` -- this description is used by the LLM. # Having a good description can help improve extraction results. name : Optional [ str ] = Field ( default = None , description = \"The name of the person\" ) hair_color : Optional [ str ] = Field ( default = None , description = \"The color of the peron's hair if known\" ) height_in_meters : Optional [ str ] = Field ( default = None , description = \"Height measured in meters\" ) There are two best practices when defining schema: Document the attributes and the schema itself: This information is sent to the LLM and is used to improve the quality of information extraction. Do not force the LLM to make up information! Above we used Optional for the attributes allowing the LLM to output None if it doesn't know the answer. info For best performance, document the schema well and make sure the model isn't force to return results if there's no information to be extracted in the text. The Extractor ‚Äã Let's create an information extractor using the schema we defined above. from typing import Optional from langchain_core . prompts import ChatPromptTemplate , MessagesPlaceholder from langchain_core . pydantic_v1 import BaseModel , Field from langchain_openai import ChatOpenAI # Define a custom prompt to provide instructions and any additional context. # 1) You can add examples into the prompt template to improve extraction quality # 2) Introduce additional parameters to take context into account (e.g.,",
            "MessagesPlaceholder from langchain_core . pydantic_v1 import BaseModel , Field from langchain_openai import ChatOpenAI # Define a custom prompt to provide instructions and any additional context. # 1) You can add examples into the prompt template to improve extraction quality # 2) Introduce additional parameters to take context into account (e.g., include metadata #    about the document from which the text was extracted.) prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"You are an expert extraction algorithm. \" \"Only extract relevant information from the text. \" \"If you do not know the value of an attribute asked to extract, \" \"return null for the attribute's value.\" , ) , # Please see the how-to about improving performance with # reference examples. # MessagesPlaceholder('examples'), ( \"human\" , \"{text}\" ) , ] ) API Reference: ChatPromptTemplate MessagesPlaceholder ChatOpenAI We need to use a model that supports function/tool calling. Please review structured output for list of some models that can be used with this API. from langchain_mistralai import ChatMistralAI llm = ChatMistralAI ( model = \"mistral-large-latest\" , temperature = 0 ) runnable = prompt | llm . with_structured_output ( schema = Person ) API Reference: ChatMistralAI Let's test it out text = \"Alan Smith is 6 feet tall and has blond hair.\" runnable . invoke ( { \"text\" : text } ) Person(name='Alan Smith', hair_color='blond', height_in_meters='1.8288') info Extraction is Generative ü§Ø LLMs are",
            ", temperature = 0 ) runnable = prompt | llm . with_structured_output ( schema = Person ) API Reference: ChatMistralAI Let's test it out text = \"Alan Smith is 6 feet tall and has blond hair.\" runnable . invoke ( { \"text\" : text } ) Person(name='Alan Smith', hair_color='blond', height_in_meters='1.8288') info Extraction is Generative ü§Ø LLMs are generative models, so they can do some pretty cool things like correctly extract the height of the person in meters",
            "even though it was provided in feet! Multiple Entities ‚Äã In most cases , you should be extracting a list of entities rather than a single entity. This can be easily achieved using pydantic by nesting models inside one another. from typing import List , Optional from langchain_core . pydantic_v1 import BaseModel , Field class Person ( BaseModel ) : \"\"\"Information about a person.\"\"\" # ^ Doc-string for the entity Person. # This doc-string is sent to the LLM as the description of the schema Person, # and it can help to improve extraction results. # Note that: # 1. Each field is an `optional` -- this allows the model to decline to extract it! # 2. Each field has a `description` -- this description is used by the LLM. # Having a good description can help improve extraction results. name : Optional [ str ] = Field ( default = None , description = \"The name of the person\" ) hair_color : Optional [ str ] = Field ( default = None , description = \"The color of the peron's hair if known\" ) height_in_meters : Optional [ str ] = Field ( default = None , description = \"Height measured in meters\" ) class Data ( BaseModel ) : \"\"\"Extracted data about people.\"\"\" # Creates a model so that we can extract multiple entities. people : List [ Person ] info Extraction might not be perfect here. Please continue to see how to use Reference Examples to improve the quality of extraction, and see the guidelines section! runnable = prompt | llm . with_structured_output ( schema = Data ) text = \"My name is",
            "people.\"\"\" # Creates a model so that we can extract multiple entities. people : List [ Person ] info Extraction might not be perfect here. Please continue to see how to use Reference Examples to improve the quality of extraction, and see the guidelines section! runnable = prompt | llm . with_structured_output ( schema = Data ) text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\" runnable . invoke ( { \"text\" : text } ) Data(people=[Person(name='Jeff', hair_color=None, height_in_meters=None), Person(name='Anna', hair_color=None, height_in_meters=None)]) tip When the schema accommodates the extraction of multiple entities , it also allows the model to extract no entities if no relevant information",
            "is in the text by providing an empty list. This is usually a good thing! It allows specifying required attributes on an entity without necessarily forcing the model to detect this entity. Next steps ‚Äã Now that you understand the basics of extraction with LangChain, you're ready to proceed to the rest of the how-to guide: Add Examples : Learn how to use reference examples to improve performance. Handle Long Text : What should you do if the text does not fit into the context window of the LLM? Handle Files : Examples of using LangChain document loaders and parsers to extract from files like PDFs. Use a Parsing Approach : Use a prompt based approach to extract with models that do not support tool/function calling . Guidelines : Guidelines for getting good performance on extraction tasks. Help us out by providing feedback on this documentation page: Previous Extracting structured output Next Guidelines Set up The Schema The Extractor Multiple Entities Next steps Community Discord Twitter GitHub Python JS/TS More Homepage Blog YouTube Copyright ¬© 2024 LangChain, Inc."
        ]
    }
]