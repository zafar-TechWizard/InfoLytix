[
    {
        "url": "https://adasci.org/product/unpacking-parallelism-practical-strategies-for-scaling-ai-workflows/",
        "title": "[Upcoming Webinar] Unpacking Parallelism: Practical Strategies for Scaling AI Workflows - Association of Data Scientists",
        "documents": [
            "[Upcoming Webinar] Unpacking Parallelism: Practical Strategies for Scaling AI Workflows - Association of Data Scientists Skip to content Memberships Close Memberships Open Memberships Individual Membership Join the world‚Äôs leading Data Science professional community. You can access both General & Premium Memberships. Learn More Corporate Membership Any corporate, organization or academic institution having common interests in the AI field can become a member of ADaSci. Learn More Accreditations Close Accreditations Open Accreditations Chartered Data Scientist‚Ñ¢ The Chartered Data Scientist (CDS) credential gives a strong understanding of advanced data science profession and in-depth, applied analytics skills. Learn More Certified Data Scientist - Associate Level Best suitable for the aspirants who want to start their career in the data science field, this certification. Learn More Certified Generative AI Engineer An upskilling-linked certification initiative designed to recognize talent",
            "skills. Learn More Certified Data Scientist - Associate Level Best suitable for the aspirants who want to start their career in the data science field, this certification. Learn More Certified Generative AI Engineer An upskilling-linked certification initiative designed to recognize talent in generative AI and large language models. Learn More Continuous Learning Close Continuous Learning Open Continuous Learning Our Latest Courses [Upcoming Workshop] Building AI Solutions with DeepSeek: A Hands-On Workshop ‚Çπ 1,743.00 Add to cart Generative AI-Powered Software Engineering with AI Coding Assistants ‚Çπ 5,226.00 Add to cart NVIDIA Inference Microservices (NIM) Mastery Course ‚Çπ 8,712.00 Add to cart Building Generative AI Applications with Amazon Bedrock ‚Çπ 5,226.00 Add to cart Hi, Welcome back! Keep me signed in Forgot Password? Sign In Don't have an account? Register Now Access all Courses Corporate Trainings Contact ‚Çπ 0.00 0 Cart Members Area Home / Continuous Learning /¬†[Upcoming",
            "to cart Building Generative AI Applications with Amazon Bedrock ‚Çπ 5,226.00 Add to cart Hi, Welcome back! Keep me signed in Forgot Password? Sign In Don't have an account? Register Now Access all Courses Corporate Trainings Contact ‚Çπ 0.00 0 Cart Members Area Home / Continuous Learning /¬†[Upcoming Webinar] Unpacking Parallelism: Practical Strategies for Scaling AI Workflows [Upcoming Webinar] Unpacking Parallelism: Practical Strategies for Scaling AI Workflows ‚Çπ 1,743.00 Original price was: ‚Çπ1,743.00. ‚Çπ 0.00 Current price is: ‚Çπ0.00. Shashank Kapadia is a seasoned ML Engineer specializing in scalable AI solutions, NLP, and ethical AI practices. [Upcoming Webinar] Unpacking Parallelism: Practical Strategies for Scaling AI Workflows quantity Add to cart Description Description Join Shashank Kapadia, Staff Machine Learning Engineer at Walmart Global Tech, for a 1.5-hour webinar on ‚ÄúUnpacking Parallelism: Practical Strategies for Scaling AI Workflows.‚Äù Learn how to optimize AI workflows,",
            "Strategies for Scaling AI Workflows quantity Add to cart Description Description Join Shashank Kapadia, Staff Machine Learning Engineer at Walmart Global Tech, for a 1.5-hour webinar on ‚ÄúUnpacking Parallelism: Practical Strategies for Scaling AI Workflows.‚Äù Learn how to optimize AI workflows, overcome scaling challenges, and implement parallelism techniques using distributed training, cloud infrastructure, and real-world case studies to enhance your AI systems‚Äô performance. Agenda: Introduction to Parallelism in AI Workflows Challenges in Scaling AI Workflows Key Strategies for Implementing Parallelism in AI Systems Distributed Training: Techniques and Tools Scaling AI Workflows with Cloud Computing and GPUs Real-World Case Studies and Applications About the speaker: Shashank Kapadia, Staff Machine Learning Engineer at Walmart Global Tech Shashank Kapadia is a seasoned Machine Learning Engineer with over a decade of experience in MLOps, NLP, Recommendation Systems, and LLMs. He",
            "Cloud Computing and GPUs Real-World Case Studies and Applications About the speaker: Shashank Kapadia, Staff Machine Learning Engineer at Walmart Global Tech Shashank Kapadia is a seasoned Machine Learning Engineer with over a decade of experience in MLOps, NLP, Recommendation Systems, and LLMs. He specializes in building scalable AI solutions, driving ML projects from ideation to deployment, and leading cross-functional teams. Passionate about ethical AI, he actively mentors and shares insights through writing. With expertise in TensorFlow, PyTorch, and cloud technologies like GCP and AWS, Shashank excels in transforming AI workflows for enhanced efficiency and user interaction. [Upcoming Webinar] Unpacking Parallelism: Practical Strategies for Scaling AI Workflows Category Continuous Learning Related products Generative AI Hands-On Course: Achieving Hyperpersonalization in BFSI ‚Çπ 2,613.00 Add to cart CDS Video Series | Section 04: Supervised and Unsupervised Learning ‚Çπ 3,920.00 Add",
            "Webinar] Unpacking Parallelism: Practical Strategies for Scaling AI Workflows Category Continuous Learning Related products Generative AI Hands-On Course: Achieving Hyperpersonalization in BFSI ‚Çπ 2,613.00 Add to cart CDS Video Series | Section 04: Supervised and Unsupervised Learning ‚Çπ 3,920.00 Add to cart Build your own Generative Adversarial Networks (GANs) from Scratch ‚Çπ 3,049.00 Add to cart CDS Video Series | Sec08. Deployment and Model Management ‚Çπ 4,791.00 Add to cart Not a member, but still want to know what we are upto? Subscribe to our Newsletter Email Start Free Trial The power of intelligence to propel humanity and make a difference Our Accrediations Chartered Data Scientist‚Ñ¢ (CDS) Certified Data Scientist - Associate Level Certified Generative AI Engineer CDS Program About CDS Exam Information Candidate Body of Knowledge (CBOK) Exam Structure Exam Cost and Registration Fees Ethical & Standards for Chartered Data Scientists (CDS) How to Earn the CDS Charter Terms &",
            "(CDS) Certified Data Scientist - Associate Level Certified Generative AI Engineer CDS Program About CDS Exam Information Candidate Body of Knowledge (CBOK) Exam Structure Exam Cost and Registration Fees Ethical & Standards for Chartered Data Scientists (CDS) How to Earn the CDS Charter Terms & Conditions For CDS‚Ñ¢ Membership Individual Membership Institutional Membership About About ADaSci Continuous Learning Team Privacy Policy Terms and Conditions Chapters Blogs Contact For Organizations Corporate Trainings CDS for Organizations Corporate Membership Journal Lattice About Review Committee Twitter Facebook-f Linkedin ¬© 2024 All rights reserved Association of Data Scientists We noticed you're visiting from India. We've updated our prices to Indian rupee for your shopping convenience. Use United States (US) dollar instead. Dismiss"
        ]
    },
    {
        "url": "https://python.langchain.com/v0.1/docs/use_cases/extraction/quickstart/",
        "title": "Quickstart | ü¶úÔ∏èüîó LangChain",
        "documents": [
            "Quickstart | ü¶úÔ∏èüîó LangChain Skip to main content This is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here . Components Integrations Guides API Reference More People Versioning Contributing Templates Cookbooks Tutorials YouTube v0.1 Latest v0.2 v0.1 ü¶úÔ∏èüîó LangSmith LangSmith Docs LangServe GitHub Templates GitHub Templates Hub LangChain Hub JS/TS Docs üí¨ Search Get started Introduction Quickstart Installation Use cases Q&A with RAG Extracting structured output Quickstart Guidelines Use Reference Examples More Chatbots Tool use and agents Query analysis Q&A over SQL + CSV More Expression Language Get started Runnable interface Primitives Advantages of LCEL Streaming Add message history (memory) More Ecosystem ü¶úüõ†Ô∏è LangSmith ü¶úüï∏Ô∏è LangGraph ü¶úÔ∏èüèì LangServe Security This is documentation for LangChain v0.1 , which is no longer actively maintained. For the current stable version, see this version ( Latest ). Use cases Extracting structured output Quickstart On this page Quickstart In this quick start, we will use chat models that are capable of function/tool calling to extract information from text. info Extraction using function/tool calling only works with models that support function/tool calling . Set up ‚Äã We will use the structured output method available on LLMs that are capable of function/tool calling . Select a model, install the dependencies for it and set up API keys! !pip install langchain # Install a model",
            "information from text. info Extraction using function/tool calling only works with models that support function/tool calling . Set up ‚Äã We will use the structured output method available on LLMs that are capable of function/tool calling . Select a model, install the dependencies for it and set up API keys! !pip install langchain # Install a model capable of tool calling # pip install langchain-openai # pip install langchain-mistralai # pip install langchain-fireworks # Set env vars for the relevant model or load from a .env file: # import dotenv # dotenv.load_dotenv() The Schema ‚Äã First, we need to describe what information we want to extract from the text. We'll use Pydantic to define an example schema  to extract personal information. from typing import Optional from langchain_core . pydantic_v1 import BaseModel , Field class Person ( BaseModel ) : \"\"\"Information about a person.\"\"\" # ^ Doc-string for the entity Person. # This doc-string is sent to the LLM as the description of the schema Person, # and it can help to improve extraction results. # Note that: # 1. Each field is an `optional` -- this allows the model to decline to extract it! # 2. Each field has a `description` -- this description is used by the LLM. # Having a good description can help improve extraction results. name : Optional [ str ] = Field ( default = None , description = \"The name of the person\" ) hair_color : Optional [ str ] = Field ( default = None , description = \"The color of the peron's hair if",
            "extract it! # 2. Each field has a `description` -- this description is used by the LLM. # Having a good description can help improve extraction results. name : Optional [ str ] = Field ( default = None , description = \"The name of the person\" ) hair_color : Optional [ str ] = Field ( default = None , description = \"The color of the peron's hair if known\" ) height_in_meters : Optional [ str ] = Field ( default = None , description = \"Height measured in meters\" ) There are two best practices when defining schema: Document the attributes and the schema itself: This information is sent to the LLM and is used to improve the quality of information extraction. Do not force the LLM to make up information! Above we used Optional for the attributes allowing the LLM to output None if it doesn't know the answer. info For best performance, document the schema well and make sure the model isn't force to return results if there's no information to be extracted in the text. The Extractor ‚Äã Let's create an information extractor using the schema we defined above. from typing import Optional from langchain_core . prompts import ChatPromptTemplate , MessagesPlaceholder from langchain_core . pydantic_v1 import BaseModel , Field from langchain_openai import ChatOpenAI # Define a custom prompt to provide instructions and any additional context. # 1) You can add examples into the prompt template to improve extraction quality # 2) Introduce additional parameters to take context into account (e.g.,",
            "MessagesPlaceholder from langchain_core . pydantic_v1 import BaseModel , Field from langchain_openai import ChatOpenAI # Define a custom prompt to provide instructions and any additional context. # 1) You can add examples into the prompt template to improve extraction quality # 2) Introduce additional parameters to take context into account (e.g., include metadata #    about the document from which the text was extracted.) prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"You are an expert extraction algorithm. \" \"Only extract relevant information from the text. \" \"If you do not know the value of an attribute asked to extract, \" \"return null for the attribute's value.\" , ) , # Please see the how-to about improving performance with # reference examples. # MessagesPlaceholder('examples'), ( \"human\" , \"{text}\" ) , ] ) API Reference: ChatPromptTemplate MessagesPlaceholder ChatOpenAI We need to use a model that supports function/tool calling. Please review structured output for list of some models that can be used with this API. from langchain_mistralai import ChatMistralAI llm = ChatMistralAI ( model = \"mistral-large-latest\" , temperature = 0 ) runnable = prompt | llm . with_structured_output ( schema = Person ) API Reference: ChatMistralAI Let's test it out text = \"Alan Smith is 6 feet tall and has blond hair.\" runnable . invoke ( { \"text\" : text } ) Person(name='Alan Smith', hair_color='blond', height_in_meters='1.8288') info Extraction is Generative ü§Ø LLMs are",
            ", temperature = 0 ) runnable = prompt | llm . with_structured_output ( schema = Person ) API Reference: ChatMistralAI Let's test it out text = \"Alan Smith is 6 feet tall and has blond hair.\" runnable . invoke ( { \"text\" : text } ) Person(name='Alan Smith', hair_color='blond', height_in_meters='1.8288') info Extraction is Generative ü§Ø LLMs are generative models, so they can do some pretty cool things like correctly extract the height of the person in meters",
            "even though it was provided in feet! Multiple Entities ‚Äã In most cases , you should be extracting a list of entities rather than a single entity. This can be easily achieved using pydantic by nesting models inside one another. from typing import List , Optional from langchain_core . pydantic_v1 import BaseModel , Field class Person ( BaseModel ) : \"\"\"Information about a person.\"\"\" # ^ Doc-string for the entity Person. # This doc-string is sent to the LLM as the description of the schema Person, # and it can help to improve extraction results. # Note that: # 1. Each field is an `optional` -- this allows the model to decline to extract it! # 2. Each field has a `description` -- this description is used by the LLM. # Having a good description can help improve extraction results. name : Optional [ str ] = Field ( default = None , description = \"The name of the person\" ) hair_color : Optional [ str ] = Field ( default = None , description = \"The color of the peron's hair if known\" ) height_in_meters : Optional [ str ] = Field ( default = None , description = \"Height measured in meters\" ) class Data ( BaseModel ) : \"\"\"Extracted data about people.\"\"\" # Creates a model so that we can extract multiple entities. people : List [ Person ] info Extraction might not be perfect here. Please continue to see how to use Reference Examples to improve the quality of extraction, and see the guidelines section! runnable = prompt | llm . with_structured_output ( schema = Data ) text = \"My name is",
            "people.\"\"\" # Creates a model so that we can extract multiple entities. people : List [ Person ] info Extraction might not be perfect here. Please continue to see how to use Reference Examples to improve the quality of extraction, and see the guidelines section! runnable = prompt | llm . with_structured_output ( schema = Data ) text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\" runnable . invoke ( { \"text\" : text } ) Data(people=[Person(name='Jeff', hair_color=None, height_in_meters=None), Person(name='Anna', hair_color=None, height_in_meters=None)]) tip When the schema accommodates the extraction of multiple entities , it also allows the model to extract no entities if no relevant information",
            "is in the text by providing an empty list. This is usually a good thing! It allows specifying required attributes on an entity without necessarily forcing the model to detect this entity. Next steps ‚Äã Now that you understand the basics of extraction with LangChain, you're ready to proceed to the rest of the how-to guide: Add Examples : Learn how to use reference examples to improve performance. Handle Long Text : What should you do if the text does not fit into the context window of the LLM? Handle Files : Examples of using LangChain document loaders and parsers to extract from files like PDFs. Use a Parsing Approach : Use a prompt based approach to extract with models that do not support tool/function calling . Guidelines : Guidelines for getting good performance on extraction tasks. Help us out by providing feedback on this documentation page: Previous Extracting structured output Next Guidelines Set up The Schema The Extractor Multiple Entities Next steps Community Discord Twitter GitHub Python JS/TS More Homepage Blog YouTube Copyright ¬© 2024 LangChain, Inc."
        ]
    },
    {
        "url": "https://python.langchain.com/v0.1/docs/use_cases/extraction/how_to/examples/",
        "title": "Use Reference Examples | ü¶úÔ∏èüîó LangChain",
        "documents": [
            "Use Reference Examples | ü¶úÔ∏èüîó LangChain Skip to main content This is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here . Components Integrations Guides API Reference More People Versioning Contributing Templates Cookbooks Tutorials YouTube v0.1 Latest v0.2 v0.1 ü¶úÔ∏èüîó LangSmith LangSmith Docs LangServe GitHub Templates GitHub Templates Hub LangChain Hub JS/TS Docs üí¨ Search Get started Introduction Quickstart Installation Use cases Q&A with RAG Extracting structured output Quickstart Guidelines Use Reference Examples More Chatbots Tool use and agents Query analysis Q&A over SQL + CSV More Expression Language Get started Runnable interface Primitives Advantages of LCEL Streaming Add message history (memory) More Ecosystem ü¶úüõ†Ô∏è LangSmith ü¶úüï∏Ô∏è LangGraph ü¶úÔ∏èüèì LangServe Security This is documentation for LangChain v0.1 , which is no longer actively maintained. For the current stable version, see this version ( Latest ). Use cases Extracting structured output Use Reference Examples On this page Use Reference Examples The quality of extractions can often be improved by providing reference examples to the LLM. tip While this tutorial focuses how to use examples with a tool calling model, this technique is generally applicable, and will work",
            "also with JSON more or prompt based techniques. from langchain_core . prompts import ChatPromptTemplate , MessagesPlaceholder # Define a custom prompt to provide instructions and any additional context. # 1) You can add examples into the prompt template to improve extraction quality # 2) Introduce additional parameters to take context into account (e.g., include metadata #    about the document from which the text was extracted.) prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"You are an expert extraction algorithm. \" \"Only extract relevant information from the text. \" \"If you do not know the value of an attribute asked \" \"to extract, return null for the attribute's value.\" , ) , # ‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì MessagesPlaceholder ( \"examples\" ) , # <-- EXAMPLES! # ‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë ( \"human\" , \"{text}\" ) , ] ) API Reference: ChatPromptTemplate MessagesPlaceholder Test out the template: from langchain_core . messages import ( HumanMessage , ) prompt . invoke ( { \"text\" : \"this is some text\" , \"examples\" : [ HumanMessage ( content = \"testing 1 2 3\" ) ] } ) API Reference: HumanMessage ChatPromptValue(messages=[SystemMessage(content=\"You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.\"), HumanMessage(content='testing 1 2 3'), HumanMessage(content='this is some text')]) Define the schema ‚Äã Let's re-use the person",
            "are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.\"), HumanMessage(content='testing 1 2 3'), HumanMessage(content='this is some text')]) Define the schema ‚Äã Let's re-use the person schema from the quickstart. from typing import List , Optional from langchain_core . pydantic_v1 import BaseModel , Field from langchain_openai import ChatOpenAI class Person ( BaseModel ) : \"\"\"Information about a person.\"\"\" # ^ Doc-string for the entity Person. # This doc-string is sent to the LLM as the description of the schema Person, # and it can help to improve extraction results. # Note that: # 1. Each field is an `optional` -- this allows the model to decline to extract it! # 2. Each field has a `description` -- this description is used by the LLM. # Having a good description can help improve extraction results. name : Optional [ str ] = Field ( . . . , description = \"The name of the person\" ) hair_color : Optional [ str ] = Field ( . . . , description = \"The color of the peron's eyes if known\" ) height_in_meters : Optional [ str ] = Field ( . . . , description = \"Height in METERs\" ) class Data ( BaseModel ) : \"\"\"Extracted data about people.\"\"\" # Creates a model so that we can extract multiple entities. people : List [ Person ] API Reference: ChatOpenAI Define reference examples ‚Äã Examples can be defined as a list of input-output pairs. Each example",
            ": Optional [ str ] = Field ( . . . , description = \"Height in METERs\" ) class Data ( BaseModel ) : \"\"\"Extracted data about people.\"\"\" # Creates a model so that we can extract multiple entities. people : List [ Person ] API Reference: ChatOpenAI Define reference examples ‚Äã Examples can be defined as a list of input-output pairs. Each example contains an example input text and an example output showing what should be extracted from the text. info This is a bit in the weeds, so feel free to ignore if you don't get it! The format of the example needs to match the API used (e.g., tool calling or JSON mode etc.). Here, the formatted examples will match the format expected for the tool calling API since that's what we're using. import uuid from typing import Dict , List , TypedDict from langchain_core . messages import ( AIMessage , BaseMessage , HumanMessage , SystemMessage , ToolMessage , ) from langchain_core . pydantic_v1 import BaseModel , Field class Example ( TypedDict ) : \"\"\"A representation of an example consisting of text input and expected tool calls. For extraction, the tool calls are represented as instances of pydantic model. \"\"\" input : str # This is the example text tool_calls : List [ BaseModel ] # Instances of pydantic model that should be extracted def tool_example_to_messages ( example : Example ) - > List [ BaseMessage ] : \"\"\"Convert an example into a list of messages that can be fed into an LLM. This code is an adapter that converts our example to a list of",
            "\"\"\" input : str # This is the example text tool_calls : List [ BaseModel ] # Instances of pydantic model that should be extracted def tool_example_to_messages ( example : Example ) - > List [ BaseMessage ] : \"\"\"Convert an example into a list of messages that can be fed into an LLM. This code is an adapter that converts our example to a list of messages that can be fed into a chat model. The list of messages per example corresponds to: 1) HumanMessage: contains the content from which content should be extracted. 2) AIMessage: contains the extracted information from the model 3) ToolMessage: contains confirmation to the model that the model requested a tool correctly. The ToolMessage is required because some of the chat models are hyper-optimized for agents rather than for an extraction use case. \"\"\" messages : List [ BaseMessage ] = [ HumanMessage ( content = example [ \"input\" ] ) ] openai_tool_calls = [ ] for tool_call in example [ \"tool_calls\" ] : openai_tool_calls . append ( { \"id\" : str ( uuid . uuid4 ( ) ) , \"type\" : \"function\" , \"function\" : { # The name of the function right now corresponds # to the name of the pydantic model # This is implicit in the API right now, # and will be improved over time. \"name\" : tool_call . __class__ . __name__ , \"arguments\" : tool_call . json ( ) , } , } ) messages . append ( AIMessage ( content = \"\" , additional_kwargs = { \"tool_calls\" : openai_tool_calls } ) ) tool_outputs = example . get ( \"tool_outputs\" ) or [ \"You have correctly",
            "# This is implicit in the API right now, # and will be improved over time. \"name\" : tool_call . __class__ . __name__ , \"arguments\" : tool_call . json ( ) , } , } ) messages . append ( AIMessage ( content = \"\" , additional_kwargs = { \"tool_calls\" : openai_tool_calls } ) ) tool_outputs = example . get ( \"tool_outputs\" ) or [ \"You have correctly called this tool.\" ] * len ( openai_tool_calls ) for output , tool_call in zip ( tool_outputs , openai_tool_calls ) : messages . append ( ToolMessage ( content = output , tool_call_id = tool_call [ \"id\" ] ) ) return messages API Reference: AIMessage BaseMessage HumanMessage SystemMessage ToolMessage Next let's define our examples and then convert them into message format. examples = [ ( \"The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it.\" , Person ( name = None , height_in_meters = None , hair_color = None ) , ) , ( \"Fiona traveled far from France to Spain.\" , Person ( name = \"Fiona\" , height_in_meters = None , hair_color = None ) , ) , ] messages = [ ] for text , tool_call in examples : messages . extend ( tool_example_to_messages ( { \"input\" : text , \"tool_calls\" : [ tool_call ] } ) ) Let's test out the prompt prompt . invoke ( { \"text\" : \"this is some text\" , \"examples\" : messages } ) ChatPromptValue(messages=[SystemMessage(content=\"You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for",
            ": [ tool_call ] } ) ) Let's test out the prompt prompt . invoke ( { \"text\" : \"this is some text\" , \"examples\" : messages } ) ChatPromptValue(messages=[SystemMessage(content=\"You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.\"), HumanMessage(content=\"The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it.\"), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'c75e57cc-8212-4959-81e9-9477b0b79126', 'type': 'function', 'function': {'name': 'Person', 'arguments': '{\"name\": null, \"hair_color\": null, \"height_in_meters\": null}'}}]}), ToolMessage(content='You have correctly called this tool.', tool_call_id='c75e57cc-8212-4959-81e9-9477b0b79126'), HumanMessage(content='Fiona traveled far from France to Spain.'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '69da50b5-e427-44be-b396-1e56d821c6b0', 'type': 'function', 'function': {'name': 'Person', 'arguments': '{\"name\": \"Fiona\", \"hair_color\": null, \"height_in_meters\": null}'}}]}), ToolMessage(content='You have correctly called this tool.', tool_call_id='69da50b5-e427-44be-b396-1e56d821c6b0'), HumanMessage(content='this is some text')]) Create an extractor ‚Äã Here, we'll create an extractor using gpt-4 . # We will be using tool calling mode, which # requires a tool calling capable model. llm = ChatOpenAI ( # Consider benchmarking with a",
            "have correctly called this tool.', tool_call_id='69da50b5-e427-44be-b396-1e56d821c6b0'), HumanMessage(content='this is some text')]) Create an extractor ‚Äã Here, we'll create an extractor using gpt-4 . # We will be using tool calling mode, which # requires a tool calling capable model. llm = ChatOpenAI ( # Consider benchmarking with a good model to get # a sense of the best possible quality. model = \"gpt-4-0125-preview\" , # Remember to set the temperature to 0 for extractions! temperature = 0 , ) runnable = prompt | llm . with_structured_output ( schema = Data , method = \"function_calling\" , include_raw = False , ) /Users/harrisonchase/workplace/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change. warn_beta( Without examples üòø ‚Äã Notice that even though we're using gpt-4, it's failing with a very simple test case! for _ in range ( 5 ) : text = \"The solar system is large, but earth has only 1 moon.\" print ( runnable . invoke ( { \"text\" : text , \"examples\" : [ ] } ) ) people=[] people=[Person(name='earth', hair_color=None, height_in_meters=None)] people=[Person(name='earth', hair_color=None, height_in_meters=None)] people=[] people=[] With examples üòª ‚Äã Reference examples helps to fix the failure! for _ in range ( 5 ) : text = \"The solar system is large, but earth has only 1 moon.\" print ( runnable . invoke ( { \"text\" : text , \"examples\" :",
            "hair_color=None, height_in_meters=None)] people=[Person(name='earth', hair_color=None, height_in_meters=None)] people=[] people=[] With examples üòª ‚Äã Reference examples helps to fix the failure! for _ in range ( 5 ) : text = \"The solar system is large, but earth has only 1 moon.\" print ( runnable . invoke ( { \"text\" : text , \"examples\" : messages } ) ) people=[] people=[] people=[] people=[] people=[] runnable . invoke ( { \"text\" : \"My name is Harrison. My hair is black.\" , \"examples\" : messages , } ) Data(people=[Person(name='Harrison', hair_color='black', height_in_meters=None)]) Help us out by providing feedback on this documentation page: Previous Guidelines Next Handle Long Text Define the schema Define reference examples Create an extractor Without examples üòø With examples üòª Community Discord Twitter GitHub Python JS/TS More Homepage Blog YouTube Copyright ¬© 2024 LangChain, Inc."
        ]
    },
    {
        "url": "https://python.langchain.com/v0.1/docs/expression_language/streaming/",
        "title": "Failed to retrieve content",
        "documents": [
            "Failed to retrieve content from the link: HTTPSConnectionPool(host='python.langchain.com', port=443): Max retries exceeded with url: /v0.1/docs/expression_language/streaming/ (Caused by SSLError(SSLError(1, '[SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:1006)')))"
        ]
    },
    {
        "url": "https://python.langchain.com/v0.1/docs/expression_language/how_to/message_history/",
        "title": "Add message history (memory) | ü¶úÔ∏èüîó LangChain",
        "documents": [
            "Add message history (memory) | ü¶úÔ∏èüîó LangChain Skip to main content This is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here . Components Integrations Guides API Reference More People Versioning Contributing Templates Cookbooks Tutorials YouTube v0.1 Latest v0.2 v0.1 ü¶úÔ∏èüîó LangSmith LangSmith Docs LangServe GitHub Templates GitHub Templates Hub LangChain Hub JS/TS Docs üí¨ Search Get started Introduction Quickstart Installation Use cases Q&A with RAG Extracting structured output Chatbots Tool use and agents Query analysis Q&A over SQL + CSV More Expression Language Get started Runnable interface Primitives Advantages of LCEL Streaming Add message history (memory) More Ecosystem ü¶úüõ†Ô∏è LangSmith ü¶úüï∏Ô∏è LangGraph ü¶úÔ∏èüèì LangServe Security This is documentation for LangChain v0.1 , which is no longer actively maintained. For the current stable version, see this version ( Latest ). Expression Language Add message history (memory) On this page Add message history (memory) The RunnableWithMessageHistory lets us add message history to certain types of chains. It wraps another Runnable and manages the chat message history for it. Specifically, it can be used for any Runnable that takes as input one of a sequence of BaseMessage a dict with a key that takes a sequence of BaseMessage a dict with a key that takes the latest message(s) as a string or sequence of BaseMessage , and a separate key that takes historical messages And",
            "and manages the chat message history for it. Specifically, it can be used for any Runnable that takes as input one of a sequence of BaseMessage a dict with a key that takes a sequence of BaseMessage a dict with a key that takes the latest message(s) as a string or sequence of BaseMessage , and a separate key that takes historical messages And returns as output one of a string that can be treated as the contents of an AIMessage a sequence of BaseMessage a dict with a key that contains a sequence of BaseMessage Let's take a look at some examples to see how it works. First we construct a runnable (which here accepts a dict as input and returns a message as output): from langchain_core . prompts import ChatPromptTemplate , MessagesPlaceholder from langchain_openai . chat_models import ChatOpenAI model = ChatOpenAI ( ) prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"You're an assistant who's good at {ability}. Respond in 20 words or fewer\" , ) , MessagesPlaceholder ( variable_name = \"history\" ) , ( \"human\" , \"{input}\" ) , ] ) runnable = prompt | model API Reference: ChatPromptTemplate MessagesPlaceholder ChatOpenAI To manage the message history, we will need: This runnable; A callable that returns an instance of BaseChatMessageHistory . Check out the memory integrations page for implementations of chat message histories using Redis and other providers. Here we demonstrate using an in-memory ChatMessageHistory as well as more persistent storage using",
            "To manage the message history, we will need: This runnable; A callable that returns an instance of BaseChatMessageHistory . Check out the memory integrations page for implementations of chat message histories using Redis and other providers. Here we demonstrate using an in-memory ChatMessageHistory as well as more persistent storage using RedisChatMessageHistory . In-memory ‚Äã Below we show a simple example in which the chat history lives in memory, in this case via a global Python dict. We construct a callable get_session_history that references this dict to return an instance of ChatMessageHistory . The arguments to the callable can be specified by passing a configuration to the RunnableWithMessageHistory at runtime. By default, the configuration parameter is expected to be a single string session_id . This can be adjusted via the history_factory_config kwarg. Using the single-parameter default: from langchain_community . chat_message_histories import ChatMessageHistory from langchain_core . chat_history import BaseChatMessageHistory from langchain_core . runnables . history import RunnableWithMessageHistory store = { } def get_session_history ( session_id : str ) - > BaseChatMessageHistory : if session_id not in store : store [ session_id ] = ChatMessageHistory ( ) return store [ session_id ] with_message_history = RunnableWithMessageHistory ( runnable , get_session_history , input_messages_key = \"input\" , history_messages_key = \"history\" , ) API Reference:",
            "{ } def get_session_history ( session_id : str ) - > BaseChatMessageHistory : if session_id not in store : store [ session_id ] = ChatMessageHistory ( ) return store [ session_id ] with_message_history = RunnableWithMessageHistory ( runnable , get_session_history , input_messages_key = \"input\" , history_messages_key = \"history\" , ) API Reference: ChatMessageHistory BaseChatMessageHistory RunnableWithMessageHistory Note that we've specified input_messages_key (the key to be treated as the latest input message) and history_messages_key (the key to add historical messages to). When invoking this new runnable, we specify the corresponding chat history via a configuration parameter: with_message_history . invoke ( { \"ability\" : \"math\" , \"input\" : \"What does cosine mean?\" } , config = { \"configurable\" : { \"session_id\" : \"abc123\" } } , ) AIMessage(content='Cosine is a trigonometric function that calculates the ratio of the adjacent side to the hypotenuse of a right triangle.') # Remembers with_message_history . invoke ( { \"ability\" : \"math\" , \"input\" : \"What?\" } , config = { \"configurable\" : { \"session_id\" : \"abc123\" } } , ) AIMessage(content='Cosine is a mathematical function used to calculate the length of a side in a right triangle.') # New session_id --> does not remember. with_message_history . invoke ( { \"ability\" : \"math\" , \"input\" : \"What?\" } , config = { \"configurable\" : { \"session_id\" : \"def234\" } } , ) AIMessage(content='I can help with math problems. What do you need",
            "is a mathematical function used to calculate the length of a side in a right triangle.') # New session_id --> does not remember. with_message_history . invoke ( { \"ability\" : \"math\" , \"input\" : \"What?\" } , config = { \"configurable\" : { \"session_id\" : \"def234\" } } , ) AIMessage(content='I can help with math problems. What do you need assistance with?') The configuration parameters by which we track message histories can be customized by passing in a list of ConfigurableFieldSpec objects to the history_factory_config parameter. Below, we use two parameters: a user_id and conversation_id . from langchain_core . runnables import ConfigurableFieldSpec store = { } def get_session_history ( user_id : str , conversation_id : str ) - > BaseChatMessageHistory : if ( user_id , conversation_id ) not in store : store [ ( user_id , conversation_id ) ] = ChatMessageHistory ( ) return store [ ( user_id , conversation_id ) ] with_message_history = RunnableWithMessageHistory ( runnable , get_session_history , input_messages_key = \"input\" , history_messages_key = \"history\" , history_factory_config = [ ConfigurableFieldSpec ( id = \"user_id\" , annotation = str , name = \"User ID\" , description = \"Unique identifier for the user.\" , default = \"\" , is_shared = True , ) , ConfigurableFieldSpec ( id = \"conversation_id\" , annotation = str , name = \"Conversation ID\" , description = \"Unique identifier for the conversation.\" , default = \"\" , is_shared = True , ) , ] , ) API Reference:",
            "\"user_id\" , annotation = str , name = \"User ID\" , description = \"Unique identifier for the user.\" , default = \"\" , is_shared = True , ) , ConfigurableFieldSpec ( id = \"conversation_id\" , annotation = str , name = \"Conversation ID\" , description = \"Unique identifier for the conversation.\" , default = \"\" , is_shared = True , ) , ] , ) API Reference: ConfigurableFieldSpec with_message_history . invoke ( { \"ability\" : \"math\" , \"input\" : \"Hello\" } , config = { \"configurable\" : { \"user_id\" : \"123\" , \"conversation_id\" : \"1\" } } , ) Examples with runnables of different signatures ‚Äã The above runnable takes a dict as input and returns a BaseMessage. Below we show some alternatives. Messages input, dict output ‚Äã from langchain_core . messages import HumanMessage from langchain_core . runnables import RunnableParallel chain = RunnableParallel ( { \"output_message\" : ChatOpenAI ( ) } ) def get_session_history ( session_id : str ) - > BaseChatMessageHistory : if session_id not in store : store [ session_id ] = ChatMessageHistory ( ) return store [ session_id ] with_message_history = RunnableWithMessageHistory ( chain , get_session_history , output_messages_key = \"output_message\" , ) with_message_history . invoke ( [ HumanMessage ( content = \"What did Simone de Beauvoir believe about free will\" ) ] , config = { \"configurable\" : { \"session_id\" : \"baz\" } } , ) API Reference: HumanMessage RunnableParallel {'output_message': AIMessage(content=\"Simone de Beauvoir believed in the existence of",
            "= \"output_message\" , ) with_message_history . invoke ( [ HumanMessage ( content = \"What did Simone de Beauvoir believe about free will\" ) ] , config = { \"configurable\" : { \"session_id\" : \"baz\" } } , ) API Reference: HumanMessage RunnableParallel {'output_message': AIMessage(content=\"Simone de Beauvoir believed in the existence of free will. She argued that individuals have the ability to make choices and determine their own actions, even in the face of social and cultural constraints. She rejected the idea that individuals are purely products of their environment or predetermined by biology or destiny. Instead, she emphasized the importance of personal responsibility and the need for individuals to actively engage in creating their own lives and defining their own existence. De Beauvoir believed that freedom and agency come from recognizing one's own freedom and actively exercising it in the pursuit of personal and collective liberation.\")} with_message_history . invoke ( [ HumanMessage ( content = \"How did this compare to Sartre\" ) ] , config = { \"configurable\" : { \"session_id\" : \"baz\" } } , ) {'output_message': AIMessage(content='Simone de Beauvoir\\'s views on free will were closely aligned with those of her contemporary and partner Jean-Paul Sartre. Both de Beauvoir and Sartre were existentialist philosophers who emphasized the importance of individual freedom and the rejection of determinism. They believed that human beings have the capacity to transcend their",
            "de Beauvoir\\'s views on free will were closely aligned with those of her contemporary and partner Jean-Paul Sartre. Both de Beauvoir and Sartre were existentialist philosophers who emphasized the importance of individual freedom and the rejection of determinism. They believed that human beings have the capacity to transcend their circumstances and create their own meaning and values.\\n\\nSartre, in his famous work \"Being and Nothingness,\" argued that human beings are condemned to be free, meaning that we are burdened with the responsibility of making choices and defining ourselves in a world that lacks inherent meaning. Like de Beauvoir, Sartre believed that individuals have the ability to exercise their freedom and make choices in the face of external and internal constraints.\\n\\nWhile there may be some nuanced differences in their philosophical writings, overall, de Beauvoir and Sartre shared a similar belief in the existence of free will and the importance of individual agency in shaping one\\'s own life.')} Messages input, messages output ‚Äã RunnableWithMessageHistory ( ChatOpenAI ( ) , get_session_history , ) Dict with single key for all messages input, messages output ‚Äã from operator import itemgetter RunnableWithMessageHistory ( itemgetter ( \"input_messages\" ) | ChatOpenAI ( ) , get_session_history , input_messages_key = \"input_messages\" , ) Persistent storage ‚Äã In many cases it is preferable to persist conversation histories. RunnableWithMessageHistory is agnostic as to",
            "for all messages input, messages output ‚Äã from operator import itemgetter RunnableWithMessageHistory ( itemgetter ( \"input_messages\" ) | ChatOpenAI ( ) , get_session_history , input_messages_key = \"input_messages\" , ) Persistent storage ‚Äã In many cases it is preferable to persist conversation histories. RunnableWithMessageHistory is agnostic as to how the get_session_history callable retrieves its chat message histories. See here for an example using a local filesystem. Below we demonstrate how one could use Redis. Check out the memory integrations page for implementations of chat message histories using other providers. Setup ‚Äã We'll need to install Redis if it's not installed already: % pip install - - upgrade - - quiet redis Start a local Redis Stack server if we don't have an existing Redis deployment to connect to: docker run -d -p 6379 :6379 -p 8001 :8001 redis/redis-stack:latest REDIS_URL = \"redis://localhost:6379/0\" LangSmith ‚Äã LangSmith is especially useful for something like message history injection, where it can be hard to otherwise understand what the inputs are to various parts of the chain. Note that LangSmith is not needed, but it is helpful.",
            "If you do want to use LangSmith, after you sign up at the link above, make sure to uncoment the below and set your environment variables to start logging traces: # os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass() Updating the message history implementation just requires us to define a new callable, this time returning an instance of RedisChatMessageHistory : from langchain_community . chat_message_histories import RedisChatMessageHistory def get_message_history ( session_id : str ) - > RedisChatMessageHistory : return RedisChatMessageHistory ( session_id , url = REDIS_URL ) with_message_history = RunnableWithMessageHistory ( runnable , get_message_history , input_messages_key = \"input\" , history_messages_key = \"history\" , ) API Reference: RedisChatMessageHistory We can invoke as before: with_message_history . invoke ( { \"ability\" : \"math\" , \"input\" : \"What does cosine mean?\" } , config = { \"configurable\" : { \"session_id\" : \"foobar\" } } , ) AIMessage(content='Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right triangle.') with_message_history . invoke ( { \"ability\" : \"math\" , \"input\" : \"What's its inverse\" } , config = { \"configurable\" : { \"session_id\" : \"foobar\" } } , ) AIMessage(content='The inverse of cosine is the arccosine function, denoted as acos or cos^-1, which gives the angle corresponding to a given cosine value.') tip Langsmith trace Looking at the Langsmith",
            ". invoke ( { \"ability\" : \"math\" , \"input\" : \"What's its inverse\" } , config = { \"configurable\" : { \"session_id\" : \"foobar\" } } , ) AIMessage(content='The inverse of cosine is the arccosine function, denoted as acos or cos^-1, which gives the angle corresponding to a given cosine value.') tip Langsmith trace Looking at the Langsmith trace for the second call, we can see that when constructing the prompt, a \"history\" variable has been injected which is a list of two messages (our first input and first output). Help us out by providing feedback on this documentation page: Previous Streaming Next Route logic based on input In-memory Examples with runnables of different signatures Persistent storage Setup LangSmith Community Discord Twitter GitHub Python JS/TS More Homepage Blog YouTube Copyright ¬© 2024 LangChain, Inc."
        ]
    }
]